{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Stochastic GD\n",
        "\n",
        "import numpy as np\n",
        "np.set_printoptions(precision=2)\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "x = np.array([[0,1],[2,6],[3,8]]) #x1, x2\n",
        "y = np.array([1,1,4])\n",
        "\n",
        "x_b = np.c_[np.ones((x.shape[0],1)),x]\n",
        "\n",
        "def cost_function(theta, x, y, N):\n",
        "  y_hat = x.dot(theta)\n",
        "  c = (1/N)*np.sum((y_hat-y)**2)\n",
        "  return c\n",
        "\n",
        "def stochastic_gradient_descent(alpha, x, y, ep=0.001, max_iter=10000):\n",
        "  converged = False\n",
        "  iter = 0\n",
        "  N = x.shape[0] # number of samples\n",
        "  print(\"Num of data = \",N)\n",
        "\n",
        "  # initial theta\n",
        "  theta =  np.random.random((x.shape[1],1))\n",
        "  print(\"Init theta.shape = \",theta.shape)\n",
        "\n",
        "  # total error, J(theta)\n",
        "  J = cost_function(theta, x, y, N)\n",
        "  print(\"First J = \",J)\n",
        "\n",
        "  # Iterate Loop\n",
        "  while not converged:\n",
        "    indices = np.arange(N)\n",
        "    np.random.shuffle(indices)  # Shuffle indices to ensure randomness\n",
        "    for i in indices:\n",
        "      xi = x[i:i+1]\n",
        "      yi = y[i:i+1]\n",
        "\n",
        "      y_hat = xi.dot(theta)\n",
        "      diff = y_hat - yi\n",
        "      grad = xi.T.dot(diff)\n",
        "\n",
        "      theta = theta - alpha * grad\n",
        "\n",
        "      # error\n",
        "      J2 = cost_function(theta, x, y, N)\n",
        "\n",
        "      if abs(J-J2) <= ep:\n",
        "          print(\"       Converged, iterations: \", iter, \"/\", max_iter)\n",
        "          converged = True\n",
        "          break\n",
        "\n",
        "      J = J2   # update error s\n",
        "      iter += 1  # update iter\n",
        "\n",
        "      if iter == max_iter:\n",
        "          print('       Max iterations exceeded!')\n",
        "          converged = True\n",
        "          break\n",
        "\n",
        "  #print(\"End converged iter = \",iter)\n",
        "  return theta\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  print(\"start main\")\n",
        "  print(x_b.shape)\n",
        "  y = y.reshape(-1,1)\n",
        "  print(y.shape)\n",
        "\n",
        "  alpha = 0.01 # learning rate\n",
        "  #Training process\n",
        "  theta = stochastic_gradient_descent(alpha, x_b, y, ep=0.000000000001, max_iter=1000000)\n",
        "  print (\"Theta = \", theta)\n",
        "\n",
        "  #predict trained x\n",
        "  xtest = np.array([[4,9]])\n",
        "  xtest_b = np.c_[np.ones((xtest.shape[0],1)),xtest]\n",
        "  y_p = xtest_b.dot(theta)\n",
        "  print(\"y predict = \",y_p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCfRhxUishWc",
        "outputId": "ce5bde8b-3797-40b4-f550-89064e3b7601"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start main\n",
            "(3, 3)\n",
            "(3, 1)\n",
            "Num of data =  3\n",
            "Init theta.shape =  (3, 1)\n",
            "First J =  26.680960520609034\n",
            "       Converged, iterations:  240507 / 1000000\n",
            "Theta =  [[ 7.]\n",
            " [15.]\n",
            " [-6.]]\n",
            "y predict =  [[13.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mini-batch GD (size = 2)\n",
        "\n",
        "import numpy as np\n",
        "np.set_printoptions(precision=2)\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "x = np.array([[0,1],[2,6],[3,8]]) #x1, x2\n",
        "y = np.array([1,1,4])\n",
        "\n",
        "x_b = np.c_[np.ones((x.shape[0],1)),x]\n",
        "\n",
        "def cost_function(theta, x, y, N):\n",
        "  y_hat = x.dot(theta)\n",
        "  c = (1/N)*np.sum((y_hat-y)**2)\n",
        "  return c\n",
        "\n",
        "def mini_batch_gradient_descent(alpha, x, y, batch_size=2, ep=0.001, max_iter=10000):\n",
        "  converged = False\n",
        "  iter = 0\n",
        "  N = x.shape[0] # number of samples\n",
        "  print(\"Num of data = \", N)\n",
        "\n",
        "  # initial theta\n",
        "  theta = np.random.random((x.shape[1],1))\n",
        "  print(\"Init theta.shape = \",theta.shape)\n",
        "\n",
        "  # total error, J(theta)\n",
        "  J = cost_function(theta, x, y, N)\n",
        "  print(\"First J = \",J)\n",
        "\n",
        "  # Iterate Loop\n",
        "  while not converged:\n",
        "    indices = np.arange(N)\n",
        "    np.random.shuffle(indices)  # Shuffle indices to ensure randomness\n",
        "    for start in range(0, N, batch_size):\n",
        "      end = min(start + batch_size, N)\n",
        "      batch_indices = indices[start:end]\n",
        "      x_batch = x[batch_indices]\n",
        "      y_batch = y[batch_indices]\n",
        "\n",
        "      y_hat = x_batch.dot(theta)\n",
        "      diff = y_hat - y_batch\n",
        "      grad = x_batch.T.dot(diff) / len(y_batch)\n",
        "\n",
        "      theta = theta - alpha * grad\n",
        "\n",
        "      # error\n",
        "      J2 = cost_function(theta, x, y, N)\n",
        "\n",
        "      if abs(J - J2) <= ep:\n",
        "          print(\"       Converged, iterations: \", iter, \"/\", max_iter)\n",
        "          converged = True\n",
        "          break\n",
        "\n",
        "      J = J2   # update error\n",
        "      iter += 1  # update iter\n",
        "\n",
        "      if iter == max_iter:\n",
        "          print('       Max iterations exceeded!')\n",
        "          converged = True\n",
        "          break\n",
        "\n",
        "  #print(\"End converged iter = \",iter)\n",
        "  return theta\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  print(\"start main\")\n",
        "  print(x_b.shape)\n",
        "  y = y.reshape(-1,1)\n",
        "  print(y.shape)\n",
        "\n",
        "  alpha = 0.01 # learning rate\n",
        "  #Training process\n",
        "  theta = mini_batch_gradient_descent(alpha, x_b, y, batch_size=2, ep=0.000000000001, max_iter=1000000)\n",
        "  print (\"Theta = \", theta)\n",
        "\n",
        "  #predict trained x\n",
        "  xtest = np.array([[4,9]])\n",
        "  xtest_b = np.c_[np.ones((xtest.shape[0],1)),xtest]\n",
        "  y_p = xtest_b.dot(theta)\n",
        "  print(\"y predict = \",y_p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTg_MytcstNs",
        "outputId": "1e9ca5d7-8817-4724-d02e-66e9265f657b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start main\n",
            "(3, 3)\n",
            "(3, 1)\n",
            "Num of data =  3\n",
            "Init theta.shape =  (3, 1)\n",
            "First J =  14.151858235162663\n",
            "       Converged, iterations:  225124 / 1000000\n",
            "Theta =  [[ 7.  ]\n",
            " [14.99]\n",
            " [-6.  ]]\n",
            "y predict =  [[13.]]\n"
          ]
        }
      ]
    }
  ]
}